\documentclass[12pt,letterpaper,noanswers]{exam}
%\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[margin=0.9in]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{multicol}
%\newcommand{\mathbf}[1]{\boldsymbol{#1}}
\pagestyle{head}
\definecolor{c02}{HTML}{FFBBBB}
\definecolor{c03}{HTML}{FFDDDD}
\header{AM 111 Problem Set 08}{}{{\colorbox{c02}{\makebox[2.8cm][l]{Due Fri Nov 10}}}\\at 12pm}
\runningheadrule
\headrule
\usepackage{diagbox}
\usepackage{graphicx} % more modern

\usepackage{amsmath} 
\usepackage{amssymb} 

\usepackage{hyperref}

\usepackage{tcolorbox}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}


\def\been{\begin{enumerate}}
\def\enen{\end{enumerate}}
\def\beit{\begin{itemize}}
\def\enit{\end{itemize}}
\def\dsst{\displaystyle}
\def\dx{\Delta x}
\hyphenation{}
\newcommand{\blank}[1]{\underline{\hspace{#1}}}

\makeatletter
\newcommand{\pyf}{%
  \begingroup\catcode`_=12
  \pyf@
}
\newcommand{\pyf@}[1]{\texttt{#1}\endgroup}
\makeatother

\newcommand{\note}[1]{\textcolor{red}{#1}} % show notes in red
%\renewcommand{\note}[1]{} % don't display notes


\begin{document}
 \pdfpageheight 11in 
  \pdfpagewidth 8.5in

\begin{itemize}
    \itemsep0pt
    \item Find the \href{https://github.com/sarah1123/ScientificComputing-APMTH111/tree/main/2023Fall/PythonFiles/08_finitedifferences}{ProblemSet08 Python template} via this link.
    \item Use this python notebook for all programming work on the problem set.  Submit the notebook to the Python assignment on Gradescope.
    \item Submit the other problems to the pdf assignment on Gradescope.
    \item Late work: Problem sets are due at noon on Fridays.  They are accepted until 5pm for all students without penalty.  In addition, you have three 29-hour late days that allow you to submit until 5pm on Saturday.  You don't need to ask to use your late days, just keep track of them for yourself.
\end{itemize}





\begin{questions}

\item Project log

\item Given a function $f$, and equally spaced points, a three-point centered difference formula for the second derivative is:
\[f''(x) = \dfrac{f(x-h)-2f(x)+f(x+h)}{h^2}-\frac{1}{12}h^2f^{(iv)}(x) + \mathcal{O}(h^n),\]
where $n\geq 3$.

If we have $x$ values $x_0, x_1, x_2, x_3, x_4$, then a second order approximation for the second derivative, evaluated at $x_2$, is given by $f''(x_2)\approx \dfrac{f(x_1) - 2f(x_2) + f(x_3)}{\Delta x^2}$ where $\Delta x = x_2-x_1$.
\begin{parts}
\item Use Richardson extrapolation to derive a higher order derivative estimate for $f''$.  Show your mathematical work.

Write this new approximation down for evaluation at the point $x_2$ in terms of $x_0, x_1, x_2, x_3, x_4, \Delta x$.

\item Use Taylor expansion to find the error term that remains after Richardson extrapolation, and show that your higher order formula is fourth order.

\item Write a Python function, \pyf{fxx_extrapolated(f,x,hlist)} to encode this 4th order method.  \pyf{f} should be a function, \pyf{x} a single $x$ value, and \pyf{hlist} an array of $h$ values.  The function should return an array of estimates to $f''(x)$ evaluated for each  $h$ in \pyf{hlist}.

\emph{Avoid using a loop in your function: vectorize your calculation.}

\end{parts}
\question (Sauer \S 5.1 Q9) Consider a forward-difference approximation for the first derivative of the form $f'(x) \approx A f(x) + Bf(x+h) + Cf(x+2h)$.

\begin{parts}
    \item Use Taylor expansion to determine the coefficients $A$, $B$, and $C$ that give the maximal order of accuracy.  

Determine what this order is.

\emph{Provide your mathematical reasoning.}

\item Write a Python function, \pyf{fx_three_point(f,x,hlist)} to encode this method.  \pyf{f} should be a function, \pyf{x} a single $x$ value, and \pyf{hlist} an array of $h$ values.  The function should return an array of estimates to $f'(x)$ where each entry corresponds to an $h$ in \pyf{hlist}.

\emph{Avoid using a loop in your function: vectorize your calculation.}
\end{parts}




% \question (minimum error)

% For centered differences, $f'(x) \approx \dfrac{f(x+h)-f(x-h)}{2h}$ and the error in the approximation is bounded above by $E(h) = \frac{1}{6}h^2f'''(c) + \frac{1}{h}\epsilon_{\text{mach}}$, with $x-h < c < x+h$.  Find an expression for the value of $h$ at which the minimum error occurs.

% Assuming $f'''(c)\approx 1$, and using $\epsilon_{\text{mach}}$ for \texttt{float64}, find the value of $h$ associated with minimum error.

% Describe what happens to the error when $h$ becomes smaller than this value.


\question (Sauer \S 5.1)
Let $f(x) = \sin x - \cos x$. 

\begin{parts}


\item  

Use \texttt{sympy} to find derivatives symbolically within Python.

\begin{verbatim}
# Example.
import sympy as sy
import numpy as np
x = sy.symbols('x')
diff = sy.diff(sy.exp(x)-sy.log(x), x)
diffx = sy.lambdify(x, diff, "numpy")
diffx(1)
\end{verbatim}

Find the first and second derivative of $f(x)$ at $x = 0$.

\item 

For the two functions you created above, create an error vector (the difference between the value from \texttt{sympy} and the value from the finite difference quotient).  

Evaluate each derivative at $x = 0$ for $\vc{h} = [0.1,0.01,0.001,...,10^{-12}]^T$


\item Plot your error results on a log-log plot.

\item Calculate an approximate $h$ value where you would expect the minimum error to occur.  Does it match what you observed?

% When val < 10**-8 we're working with val^2 < machine error.
\end{parts}

\question (numerical derivative of data)
\begin{parts}
\item Write a function \pyf{dfdx(x_data,y_data)} to approximate a first derivative from data points.  Use forward/backward differences for the two edge points and central differences for all other data points.

The function should accept an array of values $\vc{x}$ and a second array of the same length $\vc{y}$.  It should return an array of estimated derivatives.  



Avoid loops to the extent possible.


\item Load the pendulum data file \texttt{pend30.npy}.  This data was collected from a swinging pendulum with a weight at 30cm along the pendulum arm.
\begin{verbatim}
import numpy as np
import matplotlib.pyplot as plt

# load the data
t,x = np.load("pend30.npy").T
plt.plot(t,x)
\end{verbatim}

Use the function you wrote above to approximate the first derivative.

\emph{Pendulum motion is sinusoidal, so we expect a sinusoidal function for the derivative.  You should see a noisy sinusoid.}

\item There are many ways to estimate the amount of noise in the data.  

Here's one way: a sinusoidal function is approximately linear near its average value and approximately quadratic at the extremes.  

Use linear least squares to fit a quadratic curve to the data between index $90$ and index $145$.  Use the difference between this fitted curve and the data as an estimate for the error in $x$.

Once you have your error estimate, divide by $h$ to generate an estimate of the  error due to measurement error in your least squares derivative.

What percent of the derivative is this error? (this can be approximate; use the largest values of the derivative for this)



% \item Interpolate the data with cubic B-splines.

% Use \texttt{scipy.interpolate.splrep} to generate the spline and \texttt{scipy.interpolate.splev} to evaluate it at a set of input values.

% Evaluate the derivative using
% \begin{verbatim}
% splinefit = scipy.interpolate.splrep(...)
% # the 1 indicates first derivative.
% deriv = scipy.interpolate.splev(tvalues, splinefit, 1)
% \end{verbatim}

% Comment on the reasonableness of this derivative estimate.

\item \emph{Time permitting}  Repeat the derivative estimate via spline interpolation using a smoothing parameter.

You are not interpolating the data, but instead using a smoothing spline.  The smoothing parameter balances generating a smooth curve with passing close to the data points.

\begin{verbatim}
# s is the smoothing parameter.
# 0 is no smoothing
smoothsplinefit = interpolate.splrep(...,s=0.000001)
\end{verbatim}

Experiment with various levels of smoothing by adjusting $s$ (use powers of $10$ for $s$).

\item \emph{Time permitting} Comment on the reasonableness of this derivative estimate, with whatever you think is the 'best' value you find for $s$.

\end{parts}

% \question Now consider approximating the second derivative at $x_0$ instead of $x_2$.  Find a second order approximation for the 2nd derivative at $x_0$, using $x_1, x_2, x_3$.
% \begin{parts}
% \item Set $h = x_k-x_{k-1}$.  Taylor expand $f(x_0 + \delta)$ to fifth order about $x_0$.  Substitute $x_1 = x_0 + h$, $x_2 = x_0 + 2h$, $x_3 = x_0 + 3h$ to 
% \end{parts}

\question \emph{Time permitting} 

Consider the formula \[\frac{-f(x-2h) + 2f(x-h) -2f(x+h) + f(x+2h)}{2h^3}.\]  Use Taylor expansion to identify what derivative this is a formula for, and to identify the order of the method.

\question Reflection
\begin{parts}
\item When you worked on the problem set where did you get stuck or become confused?
\item What aspects of the course challenged you this week?  What did you do to address those challenges?  What topics/ideas/procedures do you not yet understand?
\item What did you understand the best this week?  What, if anything, do you understand better this week than you did in the past?
\item List the people that you worked with or consulted on the problem set problems.  This might include other students in the course, course instructors, or people who have previously taken the course.
\item Below, indicate how much of your time for this class has been doing the following activities:
	\begin{enumerate}
	\item Working on the problem set (including time in Python)
	\item Reviewing course materials, including the course textbooks
	\item Working through supplementary materials
	\item Going to office hours or lab
	\item Other (please specify)
	\end{enumerate}
\item If you used chatGPT or other AI tools, attach that information as part of this question.
\end{parts}

\end{questions}

\end{document}