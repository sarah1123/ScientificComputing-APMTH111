\documentclass[12pt,letterpaper,noanswers]{exam}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[margin=0.9in]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{multicol}
\pagestyle{head}
\header{AM 111 Class 06}{}{Linear least squares: $SVD$, p.\thepage}
\runningheadrule
\headrule
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{graphicx} % more modern
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{hyperref}

\usepackage[most]{tcolorbox}
\usepackage{listings}

\definecolor{white}{rgb}{1,1,1}
\definecolor{mygreen}{rgb}{0,0.4,0}
\definecolor{light_gray}{rgb}{0.97,0.97,0.97}
\definecolor{mykey}{rgb}{0.117,0.403,0.713}

\tcbuselibrary{listings}
\newlength\inwd
\setlength\inwd{1.3cm}
% https://tex.stackexchange.com/questions/340700/ipython-notebook-input-and-output-cells-with-listings
\newcounter{ipythcntr}
\renewcommand{\theipythcntr}{\texttt{[\arabic{ipythcntr}]}}

\newtcblisting{pyin}[1][]{%
  sharp corners,
  enlarge left by=\inwd,
  width=\linewidth-\inwd,
  enhanced,
  boxrule=0pt,
  colback=light_gray,
  listing only,
  top=0pt,
  bottom=0pt,
  overlay={
    \node[
      anchor=north east,
      text width=\inwd,
      font=\footnotesize\ttfamily\color{mykey},
      inner ysep=2mm,
      inner xsep=0pt,
      outer sep=0pt
      ] 
      at (frame.north west)
      {\refstepcounter{ipythcntr}\label{#1}In \theipythcntr:};
  }
  listing engine=listing,
  listing options={
    aboveskip=1pt,
    belowskip=1pt,
    basicstyle=\footnotesize\ttfamily,
    language=Python,
    keywordstyle=\color{mykey},
    showstringspaces=false,
    stringstyle=\color{mygreen}
  },
}
\newtcblisting{pyprint}{
  sharp corners,
  enlarge left by=\inwd,
  width=\linewidth-\inwd,
  enhanced,
  boxrule=0pt,
  colback=white,
  listing only,
  top=0pt,
  bottom=0pt,
  overlay={
    \node[
      anchor=north east,
      text width=\inwd,
      font=\footnotesize\ttfamily\color{mykey},
      inner ysep=2mm,
      inner xsep=0pt,
      outer sep=0pt
      ] 
      at (frame.north west)
      {};
  }
  listing engine=listing,
  listing options={
      aboveskip=1pt,
      belowskip=1pt,
      basicstyle=\footnotesize\ttfamily,
      language=Python,
      keywordstyle=\color{mykey},
      showstringspaces=false,
      stringstyle=\color{mygreen}
    },
}
\newtcblisting{pyout}[1][\theipythcntr]{
  sharp corners,
  enlarge left by=\inwd,
  width=\linewidth-\inwd,
  enhanced,
  boxrule=0pt,
  colback=white,
  listing only,
  top=0pt,
  bottom=0pt,
  overlay={
    \node[
      anchor=north east,
      text width=\inwd,
      font=\footnotesize\ttfamily\color{mykey},
      inner ysep=2mm,
      inner xsep=0pt,
      outer sep=0pt
      ] 
      at (frame.north west)
      {\setcounter{ipythcntr}{\value{ipythcntr}}Out#1:};
  }
  listing engine=listing,
  listing options={
      aboveskip=1pt,
      belowskip=1pt,
      basicstyle=\footnotesize\ttfamily,
      language=Python,
      keywordstyle=\color{mykey},
      showstringspaces=false,
      stringstyle=\color{mygreen}
    },
}





\newcommand{\note}[1]{\textcolor{red}{#1}} % show notes in red
\renewcommand{\note}[1]{} % don't display notes

\begin{document}
 \pdfpageheight 11in 
  \pdfpagewidth 8.5in


  
\section{Preliminaries}
\begin{itemize}
\itemsep0pt
\item Problem set 02 is due on Friday at noon.
\item Problem set 02 includes some ``time permitting'' problems.  If your time spent on the problem set reaches 8 hours in the week then you are encouraged to skip these problems.  If you are not in that situation, you are expected to complete the problems.
\item There will be a skill check in class during Class 07.  The problem info is below.
\end{itemize}



\noindent\textbf{Big picture}

Using linear least squares data fitting leads to a matrix equation.  Once that is set up, we can construct the normal equations or can use matrix decomposition to solve.

Today: solving via $SVD$ decomposition.

\vspace{0.2cm}
\hrule
\vspace{0.2cm}
\noindent \textbf{Skill check practice}
\begin{questions}
\item Given a matrix $A$ (where $A^TA$ is diagonal), find the singular values of $A$.

Example: Find the singular values of $A = \left[\begin{array}{r r}
-4 & 1 \\
1 & 4 \\
0 & 2
\end{array}\right]$

\end{questions}


\vspace{0.2cm}
\hrule
\vspace{0.2cm}

\noindent \textbf{Skill check solution}
\begin{questions}
\item $A = \left[\begin{array}{r r}
-4 & 1 \\
1 & 4 \\
0 & 2
\end{array}\right]$ so $A^TA = \left[\begin{array}{r r r}
-4 & 1 & 0 \\
1 & 4 & 2
\end{array}\right]\left[\begin{array}{r r}
-4 & 1 \\
1 & 4 \\
0 & 2
\end{array}\right] = \left[\begin{array}{r r}
17 & 0 \\
0 & 21
\end{array}\right]$.

The eigenvalues of a diagonal matrix are given by the diagonal entries: $17, 21$.  The singular values of $A$ are the square roots of the eigenvalues of $A^TA$ so are $\sqrt{17}, \sqrt{21}$.

\end{questions}

\section{Solving the least squares problem}

\subsection{Matrix decomposition: $SVD$}

We will use one more matrix decomposition to solve the least squares problem.

\subsubsection{The singular values}
See Lay \S 7.4, Heath \S 3.6, \S 4.7, Sauer \S 12.3



\begin{enumerate}
\itemsep50pt
\item Which of the following matrices are orthogonal? (Heath 3.23)
\begin{parts}
\itemsep50pt
\item $\left[\begin{array}{r r}
0 & 1 \\ 1 & 0
\end{array}\right]$
\item $\left[\begin{array}{r r}
1 & 0 \\ 0 & -1
\end{array}\right]$
\item $\left[\begin{array}{r r}
2 & 0 \\ 0 & 1/2
\end{array}\right]$
\item $\left[\begin{array}{r r}
\sqrt{2}/2 & \sqrt{2}/2 \\ -\sqrt{2}/2 & \sqrt{2}/2
\end{array}\right]$
\end{parts}

\item Show that the columns of $U$ are eigenvectors of $AA^T$.
\end{enumerate}

\vspace{1in}

\subsubsection{Reduced singular value decomposition}

Let $A\mathbf{c} = \mathbf{y}$ be overdetermined with $A$ full rank.  $A$ is $m\times n, m>n$ so rank$(A) = n$.

$A = U_1 S_1 V^T$ where $V = \left[\mathbf{v}_1 ... \mathbf{v}_n\right]$, $S = \left[\begin{array}{l r r r l}\sigma_1 & 0 & ... & & 0\\
0 & \sigma_2 & 0 & ... & 0 \\
\vdots & & \ddots & & \vdots \\
\\
0 & ... & & 0 & \sigma_n\end{array}\right]$, and $U_1 = \left[\mathbf{u}_1 ... \mathbf{u}_n \right]$

where $\mathbf{u}_i = \dfrac{1}{\sigma_i}A\mathbf{v}_i$



\begin{enumerate}[resume]
\itemsep50pt

\item Consider the column vector $\mathbf{a}$ as an $n\times 1$ matrix.  Write out its reduced singular value decomposition, finding $U_1, S_1, V$ explicitly.

\item Consider the row vector $\mathbf{a}^T$.  Why won't $\mathbf{a}^T$ have a reduced singular value decomposition?

\item Consider the figure below.  Which image represents the error in linear least squares?  What are the other two images showing?


\includegraphics[width=\textwidth]{AM111-F23-CourseNotes/img/leastsquares.png}
\end{enumerate}


\subsection{Computation}

For your reference:

\begin{pyin}
\## 21 microseconds
U1, S1, VT = np.linalg.svd(A, full_matrices=False)
sigma = np.diag(S1)
# pseudoinverse of sigma:
sigma_inv = np.linalg.pinv(sigma)
c_over = VT.T @ sigma_inv @ U1.T @ y
print("coefficients:", [c for c in c_over])
\end{pyin}
\begin{pyout}
coefficients: [1.         1.         1.         0.99999999 1.00000004 1.00000003 1.00000006 1.        ]
\end{pyout}

\begin{pyin}
\## 13.9 microseconds
c_over, res, rnk, s = linalg.lstsq(A, y)
print("coefficients:", [c for c in c_over])
\end{pyin}
\begin{pyout}
coefficients: [1.         1.         1.         1.         1.         1.00000001 0.99999999 1.00000001]
\end{pyout}



% \subsection{Other SVD facts}


% In the Euclidean norm, the matrix condition number is given by $\text{cond}(A) = \sigma_{max}/\sigma_{min}$.


\section{Total least squares}



\begin{enumerate}[resume]
\item Find the line of code where the coefficients are constructed.  What are the steps to create the vector $\mathbf{c}$?

\begin{pyin}
\## 9.4 microseconds
A1 = np.column_stack((A, y))
U1, S1, VT = np.linalg.svd(A1, full_matrices=False)
c_over_2 = -1/VT.T[-1,-1]*VT.T[0:-2,-1]
print("coefficients:", [c for c in c_over_2])

S1[-1] = 0
sigma = np.diag(S1)
Ahat = U1@sigma@VT
\end{pyin}

\begin{pyout}
coefficients: [1. 1.  1.  1. 1. 1. 0.99999999]
\end{pyout}

\end{enumerate}

\section{More practice}


\begin{enumerate}[resume]
\itemsep50pt
\item T/F: In solving a linear least squares problem $A\mathbf{c} = \mathbf{y}$ if the vector $\mathbf{y}$ lies in $\text{span}(A)$ then the error vector is $\mathbf{0}$.
\item Give an example of a model function that is non-linear in the components of $\mathbf{c}$.
\item For an overdetermined least squares problem, which is more serious: the rows of $A$ are linearly dependent, or the columns of $A$ are linearly dependent?
\item Let $\overline{\mathbf{c}}$ be the least squares  solution to $A\mathbf{c} = \mathbf{y}$ where $A = \left[\begin{array}{c c} 1 & 0 \\
1 & 1 \\
1 & 2 \\
 1 & 3\end{array}\right]$

Let $\mathbf{r} = \mathbf{y} - A\overline{\mathbf{c}}$.

Consider the vector $\left[\begin{array}{r}-1 \\ -1 \\ 1 \\ 1\end{array}\right].$  Why isn't this a possible value of $\mathbf{r}$?

Construct a vector that is a possible value of $\mathbf{r}$.
 
\end{enumerate}
\end{document}